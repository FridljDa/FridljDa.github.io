---
title: "Strategic Model Selection in Cursor: Balancing Cost and Performance"
pubDate: 2026-02-06
tags:
  - AI
  - Productivity
description: "Learn how to optimize your Cursor usage by choosing expensive models for planning and cheap models for execution, with a cost vs performance chart updated daily."
image: /images/blog/cursor-model-selection-strategy/featured.png
math: true
---

import ModelPerformanceChart from '../../components/ModelPerformanceChart';
import ModelRecommendations from '../../components/ModelRecommendations';

Checking Cursor’s [pricing](https://cursor.com/docs/models#model-pricing) and a [leaderboard](https://arena.ai/leaderboard/code) back and forth is tedious; using “the best” model for everything can lead to surprisingly high bills. A simpler approach: **one stronger model for planning, one cheaper model for execution.**

## The two-model strategy

**Planning** (understanding the task, designing steps) benefits from strong reasoning. You send context and get back a plan and a few key decisions; token volume is modest, so the extra cost is often worth it.

**Execution** (implementing the plan, writing code) can be done well by cheaper models when the plan is clear. This phase uses many more tokens, so keeping cost per token low matters.

1. Use a **premium model for planning**: start the task, get a clear plan, maybe one or two critical edits.
2. **Switch to a cheaper model for execution**: implement the rest, iterate, run tests.

You avoid both the “everything on the best model” bill and the “everything on the cheapest” quality hit.

## Why it works

Planning is **input-heavy** (lots of context in, compact plan out); execution is **output-heavy** (lots of code generated). [Arena Code](https://arena.ai/leaderboard/code) benchmarks show that mid-tier models are close to the top on code tasks at a fraction of the cost. So: strong reasoning where it matters, lower cost where most tokens are spent.

## Cost vs performance at a glance

The chart below plots **cost** (weighted $/1M tokens: 70% input, 30% output) vs **benchmark performance**. Data comes from Cursor’s pricing and public benchmarks; the [workflow](https://github.com/FridljDa/FridljDa.github.io/blob/master/.github/workflows/update-model-data.yaml) updates it daily.

<ModelPerformanceChart client:load />

**How to read it:** Lower left = cheaper/weaker, upper right = pricier/stronger. Pick a **planning** model from the upper-right and an **execution** model from the lower half.

## Recommended pairings

<ModelRecommendations client:load />

### How recommendations are selected

Recommendations follow five principles for optimal cost-performance trade-offs:

1. **Planning**: Elo ≥ 1300. **Execution**: 1200 ≤ Elo and Cost ≤ $2.5. The two sets can overlap.
2. **No dominated models**: If model B has same-or-better performance AND same-or-lower cost than model A, don't recommend A
3. **No dominated models**: If model B has same-or-lower cost AND same-or-better performance than model A, don't recommend A
4. **Efficiency ranking**: Models are ranked by Elo per dollar
5. **Top picks**: Maximum 3 models per category

Principles 3 and 4 define the **Pareto frontier**: a model qualifies only if no other model is both better-performing and cheaper. This ensures you're never choosing a strictly dominated option.

The chart shows Pareto frontiers as dashed lines (purple for planning, green for execution).

**Mathematical formulation:**

**Step 1: Minimum performance threshold**

$$
\text{Elo} \geq 1200
$$

Only models with Arena-Code or LMSYS Arena Elo scores qualify (no BigCodeBench-only models).

**Step 2: Category criteria (not mutually exclusive)**

$$
\text{Planning}: \text{Elo} \geq 1300 \quad|\quad \text{Execution}: \text{Elo} \geq 1200 \,\wedge\, \text{Cost} \leq \$2.5
$$

**Step 3: Pareto optimality**

Within each category, keep only Pareto-optimal models (not dominated):

$$
\text{Pareto-optimal} \iff \nexists \text{ model } B: 
\begin{cases}
\text{Elo}_B \geq \text{Elo}_A \\
\text{Cost}_B \leq \text{Cost}_A \\
\text{at least one } >
\end{cases}
$$

**Step 4: Rank by efficiency**

$$
\text{Efficiency} = \frac{\text{Elo}}{\text{Cost per 1M tokens}}
$$

Select top 3 models by efficiency (or fewer if frontier is smaller).

This approach guarantees recommended models represent the best cost-performance trade-offs available. The formula runs daily via [GitHub Actions](.github/workflows/update-model-data.yaml).

## Example workflow

1. **Start with the planning model.** Describe the goal, attach files, ask for a step-by-step plan.
2. **Lock in the plan.** Review, maybe one short follow-up, then switch model.
3. **Switch to the execution model.** Refer to the plan and implement step by step; do most coding here.
4. **Use the planning model only when needed.** For design decisions or subtle bugs, switch back briefly, then return to the cheaper model.

## Takeaways

- Use **one stronger model for planning** and **one cheaper model for execution** instead of one model for everything.
- The chart is updated daily—use it to check cost vs performance without tab-hopping.
- Pick planning from the upper-right of the chart, execution from the lower half, and switch as you move from planning to coding.
